---
title: "Lab 4 Assignment"
author: "ChanceYan"
date: "2025-02-18"
output: html_document
---

Load data into R

```{r, include = TRUE}
canopy <- read.csv(file = "main_analysis_table_x-site_redlining_canopy.csv")
head(canopy)
```

Show what our data looks like:

```{r, include = TRUE}
table(canopy$city,canopy$holc_grade)
xtabs(~city+holc_grade, data=canopy) # another option
```

Combine Boston and NYC into one:

```{r, include = TRUE}
nyc <- c("Bronx", "Brooklyn", "Manhattan", "Queens", "Staten Island")
bos <- c("Boston", "Cambridge", "Chelsea")
canopy$city <- ifelse(canopy$city %in% nyc, "NYC",
ifelse(canopy$city %in% bos, "Boston", canopy$city))
remove(nyc, bos)
```

Plotting our data to see what it looks like inititally:

```{r, include = TRUE}
library(ggplot2)
## set colors for plotting
holc_col <- c('#92BC6B', '#92C7C9', '#E7DC6B', '#E47D67')
holc_lab <- c('A - "Best"',
'B - "Still Desirable"',
'C - "Definitely Declining"',
'D - "Hazardous"')

#Histogram
hist(canopy$Can_P, breaks=seq(0,100,5), cex.axis=0.8,
xlab="Canopy Cover (%)", main="")

#Boxplots
ggplot(canopy, aes(x = holc_grade, y = Can_P, fill = holc_grade)) +
geom_boxplot() +
scale_fill_manual('HOLC Grade', values = holc_col, labels = holc_lab) +
labs(title = "Canopy cover by HOLC grade",
y = "Canopy Cover (%)", x = "HOLC grade") +
theme_classic()

ggplot(canopy, aes(x = city, y = Can_P)) +
geom_boxplot() +
labs(title = "Canopy cover by city",
y = "Canopy Cover (%)", x = "City") +
theme_classic()
```

Removing cities with less than 150 observations

```{r, include = TRUE}
library(tidyverse)
## filter
canopy_s <- canopy %>%
group_by(city) %>%
filter(n() >= 150) %>%
ungroup()
## new table
table(canopy_s$city,canopy_s$holc_grade)
```

**Now, let’s continue data exploration with the new simplified dataset. Try to create the figures below by modifying your code from above.**

```{r, include = TRUE}
#Histogram
hist(canopy_s$Can_P, breaks=seq(0,100,5), cex.axis=0.8,
xlab="Canopy Cover (%)", main="")

#Boxplots
ggplot(canopy_s, aes(x = holc_grade, y = Can_P, fill = holc_grade)) +
  geom_boxplot() +
  facet_wrap(~city) +
  ggtitle("Canopy Cover by HOLC grade (limited to cities with more than 150 observations)") +
  theme_classic() + 
  xlab("HOLC grade") + 
  ylab("Canopy Cover (%)") +
  guides(fill=guide_legend(title="HOLC grade"))
```

Looking at it by mean

```{r, include = TRUE}
## Mean canopy cover plot
canopy_mean <- canopy_s %>% group_by(city, holc_grade) %>%
summarise(Can_Pm = mean(Can_P))
ggplot(canopy_mean, aes(x = city, y = Can_Pm, group = holc_grade,
color = holc_grade)) +
geom_line() +
scale_color_manual('HOLC Grade', values = holc_col) +
labs(title = "Canopy cover by city",
y = "Canopy Cover (%)", x = "City") +
theme_classic()
```

**Quiz yourself: (1pt) What do you notice about the raw data? How well do you think our data will meet the assumptions of the linear model (we will again assume our data are independent and measured without error)?**

After looking at the histogram of the canopy cover percentage, I notice that the histogram is left skewed. Thus it is not normally distributed and therefore violating one of the assumptions of normality. Briefly looking at the boxplots, there seems to be a decent amount of outliers that violate the constant variance assumption, but overall if we squint it's "okay".

Question: Is percent canopy cover signif icantly different by HOLC Grade and/or City?

Statistical Model: Y~i~ = B~o~ + B~1~HOLCGrade~i~ + B~2~City~i~ + B~3~HOLCGrade~i~ : City~i~ + e~i~

Let’s assume our reference level is the HOLC grade A-Chicago factor combination

Use the information above to create a generalized linear model, assuming normal distribution of the residuals (family = “gaussian”), with both HOLC Grade and City.

```{r, include = TRUE}
mod <- glm(Can_P ~ holc_grade * city, data = canopy_s, family = "gaussian")
summary(mod)
```

**Quiz yourself: (1pt - just give an overview!) What is the summary telling you? Can you interpret the different parts? Does it make sense? Make sure you can explain each of the estimates to a peer!**

The summary shows you the differences between the different specific groups. For instance differences between cities and grades are shown, but also (with the interaction) differences between specific grades and specific cities are shown as well. Other values such as the standard error are given and the significant value (P value) to compare the values into seeing if they are statically different. The bottom few bits of information such as null deviance gives how well the response variable is explained by the null model. The residual deviance gives the deviance (badness of fit) for the full model. If Residual \< Null, then your global model is likely to be better!

Specifically, we see the differences between cities and grades (not interacting) compared to Grade A and Chicago are significantly different. There is a difference of canopy coverage between citie and grades. Looking at the interactions, we see signficant differences between Chiacgo Grade A and most other groups except for LA and grade B.

```{r, include = TRUE}
# plot!
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
plot(mod)
hist(mod$residuals)
```

**Quiz yourself: (1pt) Use your knowledge to check our assumptions. What do you think?**

Looking at the residuals vs fitted graph, the spread between the different points are somewhat distributed which means the explanatory and response variables are being explained okay. The QQ plot stops following a linear pattern near the end which makes me question the normality of the residuals. For the scale location the spread is overall random which means we have a good model same with the residuals vs leverage graph.

Looking further at the normality vs residuals graph on the histogram it seems we are okay.

**Quiz yourself: (5pts) Use the examples above to create a model list of all the candidate models (use glm rather than lm) and then compare the AICcs of those models (1pt) see if your output matches mine! HINT: don’t forget to add the AICcmodavg library!**

```{r, include = TRUE}
library(AICcmodavg)
modList <- list()
modList[["modCityGradeI"]] <- mod
modList[["modCityGrade"]] <- glm(Can_P ~ holc_grade + city, data = canopy_s, family = "gaussian")
modList[["modCity"]] <- glm(Can_P ~ city, data = canopy_s, family = "gaussian")
modList[["modGrade"]] <- glm(Can_P ~ holc_grade, data = canopy_s, family = "gaussian")
modList[["mod0"]] <- glm(Can_P ~ 1, data = canopy_s, family = "gaussian")
aictab(modList)
```

**Quiz yourself: (2pts) Which model would you pick? Why?**

I would pick the City Grade Interaction model because it has the lowest AIC value and not only does it have the lowest AIC value, but the difference between that model and the next "best model" is 55 delta units. Meaning that the difference between the two models is large. Therefore the interaction model is the best fit.

Looking at data with TukeyHSD

```{r, include = TRUE}
mod.aov <- aov(mod)
(mod.tukey <- TukeyHSD(mod.aov))
with(par(mai=c(1.2,1.5,0.5,1)),{plot(mod.tukey, las=1,cex.axis=0.4)})
```

**Quiz yourself: (1pt) What do you see? Are there any interesting interactions?**

From the different graphs produced, we see in city and grade specific, not many cross the 0 line meaning there are many signficant differences between different cities and different grades (not including interactions). We see there are a handful of significant interactions as well that do cross 0 meaning they are not signficantly different, but for the most part there are differences in interactions. All grades seem to be different from each other meanwhile only NYC and LA ar not "signifcantly" different. Possible due to them both being large cities and therefore differences between the two cities in total (ignoring grades) won't be much different. 

```{r, include = TRUE}
# create a dataframe for making predictions
df.pred <- expand.grid(unique(canopy_s$city),
unique(canopy_s$holc_grade))
colnames(df.pred) <- c("city", "holc_grade")
# make preditions of group means and confidence intervals
df.conf <- as.data.frame(predict(mod, df.pred, type = "response",
se.fit = TRUE))
df.pred <- cbind(df.pred, df.conf[1:2])
df.pred <- left_join(df.pred, canopy_mean)
# plot the precited means and CIs
ggplot() +
geom_errorbar(data = df.pred, aes(x = holc_grade, y = fit,
ymin=fit-se.fit, ymax=fit-se.fit,
color = holc_grade), width=.1) +
geom_point(data = df.pred, aes(x = holc_grade, y = fit,
color = holc_grade)) +
geom_point(data = df.pred, aes(x = holc_grade, y = Can_Pm), pch = 2) +
scale_color_manual(name = "HOLC Grade", values = c(holc_col, "black"),
labels = c(holc_lab, 'Observed mean')) +
labs(title = "Canopy cover by HOLC grade",
subtitle = "(limited to cities with more than 150 observations)",
y = "Canopy Cover (%)", x = "HOLC grade") +
facet_wrap(~ city) +
theme_classic()
```

**Quiz yourself: (2pts) How does it look like we did with our model? The black triangles are the computed means. What does this mean as far as tree canopy cover in different neighborhoods?**

It would seem our model was a very good fit for our data. As the computed means and and predicted ones are matching up to be very small range. In terms of data analysis, we see that for every city, grade A is always the highest and as the grades go down, the canopy coverage also decreases. There seems to be less canopy coverage in the lower grades. Due to historical reasons, if people are not funding these poorer neiborhoods, they will have less canopy coverage and poorer environment. The environment may be dependent in the amount of funding given to a neighborhood. 

Given our summary, there is evidence that there are differences in grades and cities and these differences in grade also vary depending on which city you are specifically looking at. 
